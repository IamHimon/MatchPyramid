{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PyTorch中矩阵乘法总结\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src='mul-known.png' width='800' height='800'/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 . Pytorch中广播机制\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "以数组A和数组B的相加为例\n",
    "\n",
    "**核心:如果相加的两个数组的shape不同, 就会触发广播机制\n",
    "1)程序会自动执行操作使得A.shape==B.shape,\n",
    "2)对应位置进行相加运算结果的shape是:A.shape和B.shape对应位置的最大值**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "有两种情况能够进行广播：\n",
    "\n",
    "1. A.ndim > B.ndim, 并且A.shape最后几个元素包含B.shape。 比如：    \n",
    "    A.shape=(2,3,4,5), B.shape=(3,4,5)  \n",
    "    A.shape=(2,3,4,5), B.shape=(4,5)  \n",
    "    A.shape=(2,3,4,5), B.shape=(5)  \n",
    "2. A.ndim == B.ndim, 并且A.shape和B.shape对应位置的元素要么相同要么其中一个是1。比如 ：  \n",
    "    A.shape=(1,9,4), B.shape=(15,1,4)  \n",
    "    A.shape=(1,9,4), B.shape=(15,1,1)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 A.ndim 大于 B.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "(2, 2, 3, 4)\n",
      "===================================\n",
      "(3, 4)\n",
      "===================================\n",
      "(2, 2, 3, 4)\n",
      "===================================\n",
      "[[[[ True  True  True  True]\n",
      "   [ True  True  True  True]\n",
      "   [ True  True  True  True]]\n",
      "\n",
      "  [[ True  True  True  True]\n",
      "   [ True  True  True  True]\n",
      "   [ True  True  True  True]]]\n",
      "\n",
      "\n",
      " [[[ True  True  True  True]\n",
      "   [ True  True  True  True]\n",
      "   [ True  True  True  True]]\n",
      "\n",
      "  [[ True  True  True  True]\n",
      "   [ True  True  True  True]\n",
      "   [ True  True  True  True]]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# a.shape=(2,2,3,4)\n",
    "a = np.arange(1,49).reshape((2,2,3,4))\n",
    "# b.shape=(3,4)\n",
    "b = np.arange(1,13).reshape((3,4))\n",
    "# numpy会将b.shape调整至(2,2,3,4), 这一步相当于numpy自动实现np.tile(b,[2,2,1,1])\n",
    "res = a + b\n",
    "print('===================================')\n",
    "print(a.shape)\n",
    "print('===================================')\n",
    "print(b.shape)\n",
    "print('===================================')\n",
    "print(res.shape)\n",
    "print('===================================')\n",
    "print(a+b == a + np.tile(b,[2,2,1,1]) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 A.ndim 等于 B.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "(4, 3)\n",
      "===================================\n",
      "(4, 1)\n",
      "===================================\n",
      "(4, 3)\n",
      "===================================\n",
      "[[ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#示例1\n",
    "# a.shape=(4,3)\n",
    "a = np.arange(12).reshape(4,3)\n",
    "# b.shape=(4,1)\n",
    "b = np.arange(4).reshape(4,1)\n",
    "# numpy会将b.shape调整至(4,3), 这一步相当于numpy自动实现np.tile(b,[1,3])\n",
    "res = a + b\n",
    "print('===================================')\n",
    "print(a.shape)\n",
    "print('===================================')\n",
    "print(b.shape)\n",
    "print('===================================')\n",
    "print(res.shape)\n",
    "print('===================================')\n",
    "print((a+b == a + np.tile(b,[1,3])) )  # 打印结果都是True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. 逐元素(Element-wise)乘法\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src='mul.png' width='500' height='500'/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "支持 矩阵，向量，标量\n",
    "\n",
    "1. a*b， 支持广播（broadcast）\n",
    "2. torch.mul()， 支持广播（broadcast）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4628, -0.2711,  1.5739],\n",
      "        [ 0.1660,  0.0560, -0.1976]])\n",
      "tensor([[-0.9069, -0.5303,  0.3433],\n",
      "        [-0.9565, -0.8471,  0.8448]])\n",
      "------\n",
      "mat1[0][0] * mat2[0][0] : tensor(0.4197)\n",
      "mat1[0][1] * mat2[0][1] : tensor(0.1438)\n",
      "------\n",
      "------\n",
      "矩阵-矩阵:\n",
      "mul(a, b):\n",
      "tensor([[ 0.4197,  0.1438,  0.5403],\n",
      "        [-0.1588, -0.0474, -0.1669]])\n",
      "torch.Size([2, 3])\n",
      "------\n",
      "a * b:\n",
      "tensor([[ 0.4197,  0.1438,  0.5403],\n",
      "        [-0.1588, -0.0474, -0.1669]])\n",
      "torch.Size([2, 3])\n",
      "------\n",
      "------\n",
      "矩阵-标量:\n",
      "tensor([[-46.2758, -27.1072, 157.3922],\n",
      "        [ 16.6020,   5.5984, -19.7567]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵*矩阵\n",
    "# 是矩阵a和b对应位相乘，a和b的维度必须相等，所以只要保证a和b的shape是broadcastable就可以。\n",
    "import torch\n",
    "mat1 = torch.randn(2, 3)\n",
    "mat2 = torch.randn(2, 3)\n",
    "print(mat1)\n",
    "print(mat2)\n",
    "print('------')\n",
    "print('mat1[0][0] * mat2[0][0] :', mat1[0][0] * mat2[0][0])\n",
    "print('mat1[0][1] * mat2[0][1] :', mat1[0][1] * mat2[0][1])\n",
    "print('------')\n",
    "print('------')\n",
    "# mul(a, b)\n",
    "mat_a = torch.mul(mat1, mat2)\n",
    "print('矩阵-矩阵:')\n",
    "print('mul(a, b):')\n",
    "print(mat_a)\n",
    "print(mat_a.shape)\n",
    "print('------')\n",
    "# a*b\n",
    "mat_b = mat1 * mat2\n",
    "print('a * b:')\n",
    "print(mat_b)\n",
    "print(mat_b.shape)\n",
    "print('------')\n",
    "print('------')\n",
    "\n",
    "# 矩阵-标量\n",
    "mat_c = torch.mul(mat1, 100)\n",
    "print('矩阵-标量:')\n",
    "print(mat_c)\n",
    "print(mat_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 向量点积\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div>\n",
    "<img src='dot.jpg' width='500' height='500'/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "向量点积是先求点乘再求和。所以需要向量维度相同。\n",
    "\n",
    "1. torch.dot()，不支持广播（broadcast）."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec1 = torch.tensor([2, 3])\n",
    "vec2 = torch.tensor([2, 1])\n",
    "vec = torch.dot(vec1, vec2)\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "1D tensors expected, but got 2D and 2D tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-dba4012cb16a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmat1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmat2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1D tensors expected, but got 2D and 2D tensors"
     ]
    }
   ],
   "source": [
    "mat1 = torch.randn(2, 3)\n",
    "mat2 = torch.randn(3, 3)\n",
    "mat = torch.dot(mat1, mat2)\n",
    "mat\n",
    "\n",
    "# 报错，只允许一维的tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 矩阵乘法\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div>\n",
    "<img src='m-mul.png' width='500' height='500'/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "1. torch.mm()，不支持broadcast\n",
    "2. torch.matmul()，支持broadcast\n",
    "3. @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm(a, b):\n",
      "tensor([[-0.2466,  0.7865,  0.0741, -0.7053],\n",
      "        [-0.1164, -0.9973,  2.6223, -1.5018]])\n",
      "torch.Size([2, 4])\n",
      "------\n",
      "matmul(a, b):\n",
      "tensor([[-0.2466,  0.7865,  0.0741, -0.7053],\n",
      "        [-0.1164, -0.9973,  2.6223, -1.5018]])\n",
      "torch.Size([2, 4])\n",
      "------\n",
      "mat1@mat2:\n",
      "tensor([[-0.2466,  0.7865,  0.0741, -0.7053],\n",
      "        [-0.1164, -0.9973,  2.6223, -1.5018]])\n",
      "torch.Size([2, 4])\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "mat1 = torch.randn(2, 3)\n",
    "mat2 = torch.randn(3, 4)\n",
    "mat_a = torch.mm(mat1, mat2)\n",
    "print('mm(a, b):')\n",
    "print(mat_a)\n",
    "print(mat_a.shape)\n",
    "# 该函数只用来计算两个2-D矩阵的矩阵乘法。\n",
    "print('------')\n",
    "\n",
    "\n",
    "mat_b = torch.matmul(mat1, mat2)\n",
    "print('matmul(a, b):')\n",
    "print(mat_b)\n",
    "print(mat_b.shape)\n",
    "print('------')\n",
    "# 有更复杂的用法\n",
    "# 两个tensors之间的矩阵乘法，具体怎么操作的，依据输入而定：\n",
    "\n",
    "mat_c = mat1@ mat2\n",
    "print('mat1@mat2:')\n",
    "print(mat_c)\n",
    "print(mat_c.shape)\n",
    "# 该函数只用来计算两个2-D矩阵的矩阵乘法。\n",
    "print('------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 三维带Batch矩阵乘法\n",
    "\n",
    "1. torch.bmm() 不支持broadcast操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bmm(a, b):\n",
      "tensor([[[ 2.8325, -2.5959, -0.4154,  1.5308],\n",
      "         [ 3.5484, -1.4033, -0.2714,  0.7386]],\n",
      "\n",
      "        [[ 0.4989, -2.6536,  1.0311,  2.5753],\n",
      "         [ 1.7673, -2.2543, -2.8876,  0.4661]],\n",
      "\n",
      "        [[-0.6346, -1.5044,  1.5649,  1.4992],\n",
      "         [-0.9662,  1.0395,  1.5298, -0.7813]],\n",
      "\n",
      "        [[ 2.2554,  0.2765, -2.6558, -1.9617],\n",
      "         [ 2.4201,  2.1655,  1.0596,  0.4645]],\n",
      "\n",
      "        [[ 0.1871,  0.3360, -1.3293,  1.7182],\n",
      "         [ 0.2896,  0.7587, -1.6035,  0.0417]]])\n",
      "torch.Size([5, 2, 4])\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "mat1 = torch.randn(5,2, 3)\n",
    "mat2 = torch.randn(5, 3, 4)\n",
    "mat_a = torch.bmm(mat1, mat2)\n",
    "print('bmm(a, b):')\n",
    "print(mat_a)\n",
    "print(mat_a.shape)\n",
    "print('------')\n",
    "# 由于神经网络训练一般采用mini-batch，经常输入的是三维带batch矩阵\n",
    "# 该函数的两个输入必须是三维矩阵且第一维相同（表示Batch维度），不支持broadcast操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 矩阵与向量相乘\n",
    "\n",
    "1. torch.mv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5008, -1.5133])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = torch.randn(2, 3)\n",
    "vec = torch.randn(3)\n",
    "res = torch.mv(mat, vec)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.3  matmul用法\n",
    "\n",
    "1. 如果两个张量都是一维的，则返回 点积（标量）\n",
    "2. 如果两个参数都是二维的，则返回 矩阵乘积。\n",
    "3. 如果第一个参数是 1 维，第二个参数是 2 维，为了矩阵乘法的目的，在第一维上加 1（达到扩充维度的目的）， 矩阵计算完成之后，第一维加上的 1 将会被删掉。\n",
    "4. 如果第一个参数是 2 维，第二个参数是 1 维，就是矩阵和向量相乘。\n",
    "5. 支持复杂高维广播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "vec1 = torch.tensor([2, 3])\n",
    "vec2 = torch.tensor([2, 1])\n",
    "vec = torch.matmul(vec1, vec2)\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. \n",
    "# 同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4932,  0.9520, -1.5402])\n",
      "tensor([[-0.4932,  0.9520, -1.5402]])\n",
      "tensor([[ 1.5088,  0.3568, -1.1389, -1.1866],\n",
      "        [ 0.9095, -0.9637,  0.3556, -0.8225],\n",
      "        [ 0.4509, -0.0729,  0.6477, -0.0493]])\n",
      "tensor([-0.5728, -0.9811, -0.0973, -0.1218])\n",
      "tensor([[-0.5728, -0.9811, -0.0973, -0.1218]])\n"
     ]
    }
   ],
   "source": [
    "# 3. \n",
    "vec1 = torch.randn(3)\n",
    "vec2 = vec1.reshape(1, -1)\n",
    "mat1 = torch.randn(3, 4)\n",
    "print(vec1)\n",
    "print(vec2)\n",
    "print(mat1)\n",
    "mat = torch.matmul(vec1, mat1)\n",
    "mat2 = torch.matmul(vec2, mat1)\n",
    "print(mat)\n",
    "print(mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7485, -1.3445,  0.4366, -1.4827],\n",
      "        [ 0.8077, -0.6766,  2.0069, -1.6549],\n",
      "        [ 0.6546, -0.1501,  0.9207,  0.4119]])\n",
      "tensor([-0.0413,  0.9223,  0.3687,  0.9711])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-2.4467, -1.5246,  0.5740])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. \n",
    "mat1 = torch.randn(3, 4)\n",
    "vec1 = torch.randn(4)\n",
    "print(mat1)\n",
    "print(vec1)\n",
    "mat = torch.matmul(mat1, vec1)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.1 batched matrix x broadcasted vector\n",
    "a = torch.randn(10, 3, 4)\n",
    "b = torch.randn(4)\n",
    "matmul_a_b = torch.matmul(a, b)\n",
    "matmul_a_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.2 batched matrix x batched matrix\n",
    "c = torch.randn(10, 3, 4)\n",
    "d = torch.randn(10, 4, 5)\n",
    "matmul_c_d = torch.matmul(c, d)\n",
    "matmul_c_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.3  batched matrix x broadcasted matrix\n",
    "m = torch.randn(10, 3, 4)\n",
    "n = torch.randn(4, 5)\n",
    "matmul_m_n = torch.matmul(m, n)\n",
    "matmul_m_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 3, 5])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.4 最后两维会广播\n",
    "tensor1 = torch.randn(10, 1, 3, 4)\n",
    "tensor2 = torch.randn(2, 4, 5)\n",
    "matmul_k_p = torch.matmul(tensor1, tensor2)\n",
    "matmul_k_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
